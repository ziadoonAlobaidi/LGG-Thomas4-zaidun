{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGRCdv2_63Fm"
   },
   "source": [
    "# Linear regression\n",
    "\n",
    "Now that we've created our first learning machine model, let's see how it works under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4IHjwpyJmYz0"
   },
   "source": [
    "## How does it work?\n",
    "Here comes a part that some of you fear: Mathematics!    \n",
    "\n",
    "But don't worry, you'll see that it's not that complicated.\n",
    "\n",
    "In this notebook, everytime we will talk about programming variables, we will format the names like `this`. \n",
    "For mathematical variables and functions, we'll be formatting them like $this$.\n",
    "\n",
    "### How to calculate the y-axis from the x-axis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPLnFMBxs4C9"
   },
   "source": [
    "A linear model is in fact based on a simple [affine function.](https://fr.wikipedia.org/wiki/Fonction_affine) !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XTFIZfapioo"
   },
   "source": [
    "$$f(x) = ax + b$$\n",
    "or ...\n",
    "$$y = f(x) = ax + b$$\n",
    "or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "y = a*x + b\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a function `f` which receives as a parameter the variables `x`,`a` and `b` and returns `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,a,b):\n",
    "    return a*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will allow us to create a straight line that passes through all the points as well as possible. For the moment, we do not know the value of parameters $a$ and $b$, so it is impossible to draw a good straight line on the scatter plot, unless we choose parameters at random. And that is what we are going to do."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nl6jA7JTdoO8"
   },
   "source": [
    "\n",
    "The linear model with random parameters would look something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6Va6jerdvjC"
   },
   "source": [
    "![image.png](../assets/random_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2x4DbSIbeQNA"
   },
   "source": [
    "But we want to achieve this result:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w7hNlrnOezsy"
   },
   "source": [
    "![](../assets/trained_bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mKo1rgh_fA7O"
   },
   "source": [
    "And it will be the role of the machine to learn how to find these values ($a$ and $b$) by minimizing the cost function that we will see in detail in the next chapter.\n",
    "\n",
    "Before we do that, we need to look at the small problem we have with this method. The function as written above only take one element, $x$. However, in the next sections, we are going to have multiple values for $x$, in a vector $X$. We'll thus denote a single item with an index $i$, like $x^{i}$. If we execute the function as is, we would have to make a loop for each element $x^{i}$ of our dataset. \n",
    "\n",
    "$$\n",
    "X = \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}\\\\\n",
    "x^{(2)}\\\\\n",
    "x^{(3)}\\\\\n",
    "... \\\\\n",
    "x^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This can be very expensive in terms of machine resources. If your dataset is very large, it will take a lot of time to train your model. \n",
    "\n",
    "To solve this problem, it is customary to use something you are beginning to know, matrices! \n",
    "\n",
    "The matrices allow us to perform the function only once on our entire dataset. \n",
    "\n",
    "The matrix writing of $f(x)=ax+b$ is written like this:\n",
    "$$ F = X \\cdot \\theta$$\n",
    "\n",
    "As these are matrices that contain all the data, by convention, we put them in uppercase.\n",
    "\n",
    "The variable $F$ will contain a matrix with the set of predictions of $x^{(i)}$. \n",
    "\n",
    "$$ \n",
    "F \\\\\n",
    "\\begin{bmatrix}\n",
    "f(x^{(1)})\\\\\n",
    "f(x^{(2)})\\\\\n",
    "f(x^{(3)})\\\\\n",
    "... \\\\\n",
    "f(x^{(m)})\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $\\theta$ (pronounced theta) will contain a vector with the values $a$ and $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\theta \\\\\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $X$ will contain a matrix with two dimensions, one dimension with the value of $x^{(i)}$ and another dimension with 1's everywhere.  Why? Because we have to multiply our $X$ and $\\theta$ matrices. Remember [matrix multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html)?\n",
    "$$ \n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "x^{(1)} && 1\\\\\n",
    "x^{(2)} && 1\\\\\n",
    "x^{(3)} && 1\\\\\n",
    "... \\\\\n",
    "x^{(m)} && 1\\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "a \\\\\n",
    "b \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "![](../assets/dot_mat.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which amounts to writing this for $y^{(1)}$ (and for every other $y^{(i)}$): \n",
    "$$ y^{(1)} = x^{(1)}* a + 1 * b$$\n",
    "\n",
    "And if we simplify:\n",
    "$$ y^{(1)} = ax^{(1)} + b$$\n",
    "\n",
    "\n",
    "So we are back to our original function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "1. Create a variable `X` which contains a matrix of shape `(30,2)` with a column filled with values of our dataframe (the same as the first notebook) and then another one with 1's. \n",
    "2. Create the `theta` variable which contains a vector with 2 random values.\n",
    "3. Create a variable `F` which contains a multiplication of the matrix `X` with the `theta` vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/salary_data.csv\")\n",
    "\n",
    "x = df.drop(columns=[\"Salary\"]).to_numpy()\n",
    "y = df.Salary.to_numpy().reshape(-1 , 1)\n",
    "\n",
    "X = np.hstack((x, np.ones(x.shape)))\n",
    "np.random.seed(0) \n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "F = X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)\n",
    "print(\"Shape of F: \", F.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `model` function that receives as parameter `X` and `theta`.  The function must compute and return `F`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, theta):\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `y_pred` variable, call the `model` function with `X` and  `theta` and assign it to `y_pred`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model(X, theta)\n",
    "print(\"Shape of y_pred: \", y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know how to apply our model to our entire dataset. Now we have to know how to find the right values for $a$ and $b$. For that we will have to calculate the average of all our errors with a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKaQ28Hafn_h"
   },
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRBuUaU2ftck"
   },
   "source": [
    "The cost function allows us to evaluate the performance of our model by measuring the errors between the prediction and the actual value. The question we ask ourselves is: How to measure these errors?\n",
    "\n",
    "Imagine that you have 4 years of experience and that you earn $110000€$ per year. Your machine learning model predicts that this salary is worth $€90000$. You can conclude that your model therefore makes an error of $90000 - 110000 = -20000 €$.\n",
    "\n",
    "Thus, you could say that to measure your errors, you have to calculate the difference $f(x)-y$. However, if your prediction $f(x)$ is less than $y$, then this error is negative (as in the example above), and it is not very practical to minimize this function.\n",
    "\n",
    "So, to measure the errors between the $f(x)$ predictions and the target values $y$ of the dataset, we calculate the square of the difference: $(f(x)-y)^2$. This, by the way, is what is called the Euclidean norm, which represents the direct distance between $f(x)$ and $y$ in Euclidean geometry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJFTEvD8wb7b"
   },
   "source": [
    "\n",
    "\n",
    "![image.png](../assets/eucli.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xa-c_X4xt2vO"
   },
   "source": [
    "But this is not enough. Indeed, we have the error of a single example. But we must have the average of all the errors of all the points. \n",
    "\n",
    "We could write it like this: \n",
    "\n",
    "\n",
    "\n",
    "$$MSE(a,b) = {\\dfrac{(f(x^{(1)})- y^{(1)})^2 + (f(x^{(2)})- y^{(2)})^2  + ... +(f(x^{(m)})- y^{(m)})^2}{m}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1GUWy24S5b27"
   },
   "source": [
    "Why $MSE$? Because this function is called **Mean Squared Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImWIompHxm2Q"
   },
   "source": [
    "By convention this function is written in the following way, adding a coefficient $\\frac{1}{2}$ to simplify a derivative calculation that will come later.\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (f(x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "Or\n",
    "\n",
    "$$ MSE(a, b) = {\\dfrac{1}{2m}} \\sum _ {i=1}^m (ax^{(i)} +b - y^{(i)})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as we work with matrices, we also have to transcribe our formula which becomes: \n",
    "\n",
    "$$MSE(\\theta) = \\frac {1}{2m}  \\sum (X \\cdot \\theta - Y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or something **similar** to this: \n",
    "\n",
    "```py\n",
    "MSE = 1/(2*m) * sum((X * theta - y)**2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47zckFxMQiVs"
   },
   "source": [
    "**Exercise:** Create a `MSE` function that receives as parameter `X, y and theta` using the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X,theta)\n",
    "    return 1/(2*m) * np.sum(y_pred - y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRRlqkpmQdk7"
   },
   "outputs": [],
   "source": [
    "error = MSE(X,y,theta)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_2cfr2DOB9X"
   },
   "source": [
    "### Minimize the cost function\n",
    "\n",
    "If the cost function is omitted with respect to the parameter, it looks something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ASRXsJyO__ic"
   },
   "source": [
    "![image.png](../assets/convexe.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55-cHhR_DBNI"
   },
   "source": [
    "The aim is therefore to reach the lowest point of the curve, i.e. the lowest possible sum of errors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqyzIhPKDMc_"
   },
   "source": [
    "![image.png](../assets/gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, there are several function minimization algorithms, such as the least squares method or **gradient descent**. We will focus here on gradient descent because it is one of the most widely used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gmDtf6l-f7T"
   },
   "source": [
    "Gradient descent is an iterative algorithm which therefore proceeds by progressive improvements. For a linear problem, this algorithm needs to have two hyper-parameters:\n",
    "\n",
    "**1. The number of iterations:** As its name indicates, this is the parameter that will determine the number of iterations.\n",
    "\n",
    "**2. The learning rate:** This is the length of the step between each iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KMJ0V9J6HQZa"
   },
   "source": [
    "![learningrate](../assets/gradient_descent_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to clearly define the learning rate. If you set a high value, the algorithm will be faster, but you risk never reaching the lowest point of the curve, the steps being too big. Our model will never be able to work since it cannot find the minimum of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TASxiF6zHnq0"
   },
   "source": [
    "![](../assets/gradient_descent_2.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, if you set a small value, then the algorithm will find the lowest point of the curve, but it will be slower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9on9fx_9HhYb"
   },
   "source": [
    "![learning rate](../assets/gradient_descent_3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each iteration, we will have to calculate the regression slope.\n",
    "\n",
    "![](../assets/derivative.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in mathematics we calculate a slope with a [partial derivative](https://en.wikipedia.org/wiki/Partial_derivative#:~:text=In%20mathematics%2C%20a%20partial%20derivative,vector%20calculus%20and%20differential%20geometry.). The symbol used to denote partial derivatives is $\\partial$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac {\\partial MSE(\\theta) }{\\partial \\theta}  = \\frac {1}{m} X^T \\cdot (X \\cdot \\theta - Y)$$\n",
    "\n",
    "The  $^T$ in $X^T$ is to transpose the matrix, just like in numpy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could translate this into code like this:\n",
    "\n",
    "```py\n",
    "1/m * X.T.dot(model(X, theta) - y)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X, theta)\n",
    "    return 1/m * X.T.dot(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have to write the gradient descent. \n",
    "\n",
    "$$\\theta = \\theta - \\eta *  \\frac {\\partial MSE(\\theta) }{\\partial \\theta}$$\n",
    "\n",
    "The variable $\\eta$ is the learning rate. So at each iteration, we redefine $\\theta$. We do: `theta` - `learning_rate` multiplied by the partial derivative of mean squared error. You could translate this into code like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```py\n",
    "theta = theta - learning_rate * grad(X, y, theta)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "1. Create a `gradient_descent` function that receives as parameter `X`, `y`, `theta`, `learning_rate` and `n_iterations`.\n",
    "2. In the function, create a variable `cost_history` with a matrix filled with 0 and which has a length of `n_iterations`. We will use it to display the plot of the model learning process.\n",
    "3. Create a loop that iterates up to `n_iterations`.\n",
    "4. In the loop, update `theta` with the formula of the gradient descent (the example above).\n",
    "5. In the loop, update `cost_history[i]` with the values of `MSE(X,y,theta)`.\n",
    "6. return `theta` and `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    \n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(0, n_iterations):\n",
    "        theta = theta - learning_rate * grad(X, y, theta) \n",
    "        cost_history[i] = MSE(X, y, theta) \n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model\n",
    "\n",
    "Now that we know which algorithm is used to minimize the cost function, we train our model.   \n",
    "We define a number of iterations, and a learning step $\\alpha$, and here we go!\n",
    "\n",
    "Once the model is trained, we observe the results compared to our dataset.\n",
    "\n",
    "**Exercise:** Create variables `n_iterations` and `learning_rate`. \n",
    "The learning rate and the number of iterations are defined by trying around a little bit. You have to try several things, there is no magic number. However, starting with 1000 iterations and a learning rate of 0.01 is a good basis to start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `theta_final`, `cost history` and call `gradient_descent()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)\n",
    "print(theta_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "1. Create a `predictions` variable that will store the result of `model(X, theta_final)`.\n",
    "2. Use matplotlib to display a scatter plot with the data and the target.\n",
    "3. On the same graph, use the `plot` method to display your predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "predictions = model(X, theta_final)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, predictions, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this:\n",
    "\n",
    "![](../assets/final_theta.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not, change the learning rate and the number of iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning curves\n",
    "To check if our gradient descent algorithm worked well, we observe the evolution of the cost function through iterations. We are supposed to obtain a curve that decreases with each iteration until it stagnates at a minimal level (close to zero). If the curve does not follow this pattern, then the learning rate may be too high, we should take a smaller step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "1. Plot the `cost_history` with respect to the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(n_iterations), cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have something like this:\n",
    "\n",
    "![](../assets/learning_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this plot, we can see that after 400 iterations, the model no longer learns and becomes constant. We can thus redefine the maximum number of iterations to 400."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "To evaluate the real performance of our model with a popular metric (for your boss, client, or colleagues) we can use the coefficient of determination, also known as $R^2$. It comes from the method of least squares. The closer the result is to 1, the better your model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred)**2).sum()\n",
    "    v = ((y - y.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_determination(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The end\n",
    "Ok ok, you just built your own model of linear regression, do you realize that? \n",
    "This part was a bit theoretical, but it's essential to understand how it works.\n",
    "\n",
    "![tired.gif](../assets/tired.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that with matrices, the multiple linear regression case also doesn't change much in the way of proceeding. The matrix writing remains very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will load our dataset. This is a fake dataset for the example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./data/data_multi.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have 100 rows, 2 features and 1 target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform to matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\\\ Y = X \\cdot \\theta \\\\\n",
    "$$\n",
    "The $Y$ vector is the same too\n",
    "$$ Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "y^{(3)}\\\\\n",
    "... \\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $X$ matrix will have as many dimensions as there are features +1  (n+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1, x^{(1)}_2, ..., x^{(1)}_{n}, 1\\\\\n",
    "x^{(2)}_1, x^{(2)}_2, ..., x^{(2)}_{n}, 1\\\\\n",
    "x^{(3)}_1, x^{(3)}_2, ..., x^{(3)}_{n}, 1\\\\\n",
    "x^{(m)}_1,x^{(m)}_2, ..., x^{(m)}_{n}, 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theta vector will have as many lines as there are parameters +1 (for the constant). \n",
    "$$ \\theta =\n",
    "\\begin{bmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "... \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our case with our dataset, we can write it like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "y^{(3)}\\\\\n",
    "... \\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1, x^{(1)}_2, 1\\\\\n",
    "x^{(2)}_1, x^{(2)}_2, 1\\\\\n",
    "x^{(3)}_1, x^{(3)}_2, 1\\\\\n",
    "x^{(m)}_1,x^{(m)}_2,  1\\\\\n",
    "\\end{bmatrix}\n",
    "\\cdot\n",
    "\\begin{bmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a variable `X` which contains a matrix of shape `(100,3)` with two column's filled with values of our dataframe and then another one with 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns=[\"y\"]).to_numpy()\n",
    "ones = np.ones((X.shape[0],1))\n",
    "X = np.hstack((features, ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Check that your matrix is of shape `(100,3)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create the theta vector with three random values. Your vector must be of shape \n",
    "`(3,1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.rand(3).reshape(-1, 1)\n",
    "print(theta)\n",
    "print(theta.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit the model\n",
    "### Define your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `model` function that receives as parameter `X` and `theta`. The function must return the computed predictions `y_pred`. This is exactly the same model as last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, theta):\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well we have the model, the $\\theta$ vector, the $X$ matrix. What are we missing? The cost function of course!\n",
    "And you know what? This too is exactly the same MSE function from last time. \n",
    "\n",
    "$$MSE(\\theta) = \\frac {1}{2m}  \\sum (X \\cdot \\theta - Y)^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a MSE function that receives as parameters `X`, `y` and `theta` using the example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X,y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X,theta)\n",
    "    return 1/(2*m) * np.sum((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = MSE(X,y,theta)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "It's time to find the minimum of our function. Well again, nothing changes compared to the last time. \n",
    "\n",
    "$$ \\frac {\\partial MSE(\\theta) }{\\partial \\theta}  = \\frac {1}{m} X^T \\cdot (X \\cdot \\theta - Y)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `grad` function that receives as parameter `X`, `y`, `theta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X, theta)\n",
    "    return 1/m * X.T.dot(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:**\n",
    "\n",
    "1. Create a `gradient_descent` function that receives as parameter `X`, `y`, `theta`, `learning_rate`, `n_iterations`.\n",
    "2. In the function, create a variable `cost_history` with a matrix filled with 0 and which has a length of `n_iterations`. We will use it to display the histogram of the model learning process.\n",
    "3. Create a loop that iterates up to `n_iterations`.\n",
    "4. In the loop, update `theta` with the formula of the gradient descent (the example above).\n",
    "5. In the loop, update `cost_history[i]` with the values of `MSE(X,y,theta)`.\n",
    "6. return `theta` and `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    cost_history = np.zeros(n_iterations)\n",
    "    \n",
    "    for i in range(0, n_iterations):\n",
    "        theta = theta - learning_rate * grad(X, y, theta) \n",
    "        cost_history[i] = MSE(X, y, theta) \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `n_iterations` and `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `theta_final`, `cost_history` and call `gradient_descent()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)\n",
    "print(theta_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Create a `predictions` variable that contains `model(X, theta_final)`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X, theta_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Plot your predictions in 3D and the true values of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.view_init(180, 180)\n",
    "ax.scatter(X[:,0], X[:,1], y)\n",
    "ax.scatter(X[:,0], X[:,1], predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Plot `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_history)), cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred)**2).sum()\n",
    "    v = ((y - y.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_determination(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "\n",
    "You are now able to create a multiple variable regression model from scratch, well, from the matrix!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://media.giphy.com/media/W9lzJDwciz6bS/giphy.gif\">"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de linearregression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
