{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Polynomial regression\n",
    "Polynomial regression will allow us to build a nonlinear model by adopting and fitting a polynomial.\n",
    "\n",
    "We have seen how to solve a linear problem. But in everyday life, we meet a lot of other models that are not linear, which are made of curves, sinusoids etc...\n",
    "\n",
    "If we observe the contamination rate of a pandemic, the line will not be linear but will probably look like an exponential curve.\n",
    "\n",
    "For example, if you apply a linear model on these data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![polynomial](../assets/polynom_1.JPG)\n",
    "\n",
    "We can see that the bias of our predictions will be important.   \n",
    "We cannot say that our model is effective.\n",
    "\n",
    "But the following model already inspires me much more confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../assets/poly_2.JPG)\n",
    "\n",
    "The polynomial can have several degrees, the more degrees it has, the more it will be able to solve complex problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/1/16/Lsf.gif\" />\n",
    "\n",
    "Source <a href=https://upload.wikimedia.org/wikipedia/commons/1/16/Lsf.gif >Wikipedia</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although polynomial models allow us to model relationships of non-linear shapes, they belong to the family of linear models. In the term \"linear model\", the adjective \"linear\" refers to the parameters of the model and the fact that their effects are added together. This is indeed the case here. Moreover, linear regression is a polynomial of degree 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables studied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will load our dataset. This is a fake dataset for the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/poly.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we now have 200 rows, 1 feature and 1 target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create the `X` and `y` variables and define which column will be the target and which column will be the feature. \n",
    "They must be of type `numpy.ndarray`. Our variable `X` therefore has one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"y\"]).to_numpy()\n",
    "y = df.y.to_numpy().reshape(-1 , 1)\n",
    "print(\"Shape of X: \", X.shape)\n",
    "print(\"Shape of y: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use matplotlib (or other) to display the dataset as a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Show correlation coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the coefficient of correlation remains important even if the dataset is not perfectly linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You now know the process!\n",
    "\n",
    "**Exercise:** Import `train_test_split` from sklearn and split the dataset and create the variables `X_train`, `X_test`, `y_train`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load and fit the model (with Sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time there is a little change. \n",
    "We have a single feature in our dataset. The polynomial model is a special case of multiple regression. So we need several features to be able to apply polynomial regression. And these features, we'll have to add them ourselves. By the way, this way of doing things has a name: feature engineering.\n",
    "\n",
    "\n",
    "Let's imagine that we want to have a 2-degree polynomial regression. \n",
    "So we will need to add a feature. \n",
    "This feature is simply an exponent of $x$.  \n",
    "\n",
    "$[ x, x^2]$\n",
    "\n",
    "So $x^2$ is the new feature.\n",
    "\n",
    "If you want a 3-degree polynomial model, you will have to add 2 features in this case.\n",
    "\n",
    "$[x, x^2, x^3]$\n",
    "\n",
    "To do this, we will need to create a pipeline. \n",
    "A pipeline is a processing chain that will execute a set of functions and pass arguments between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we need to define the number of degrees.   \n",
    "**Exercise:** Create a `degree` variable with 1 as value. (We will change this value later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a pipeline with sklearn.This pipeline must contain the `PolynomialFeatures` and `LinearRegression` classes. Don't forget to set the number of degrees for the `PolynomialFeatures`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyreg=make_pipeline(\n",
    "    PolynomialFeatures(degree),\n",
    "    LinearRegression(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Fit your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyreg.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Use a scatter plot and display your predictions on `X_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_train,y_train)\n",
    "plt.scatter(X_test, polyreg.predict(X_test),color=\"red\")\n",
    "plt.title(\"Polynomial regression with degree \"+str(degree))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a straight line it is because we have set the number of degrees to one. This confirms that the linear regression is indeed a polynomial model of degree 1.\n",
    "\n",
    "**Exercise:** Change the number of degrees and train your model again. You must try to fit the curve as well as possible while limiting the number of degrees, to save some resources from your machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again a few changes. This time we'll just have to add new features manually. \n",
    "\n",
    "### Transform to matrix\n",
    "\n",
    "$$\n",
    "\\\\ Y = X \\cdot \\theta \\\\\n",
    "$$\n",
    "\n",
    "The $Y$ vector is the same too\n",
    "\n",
    "$$Y =\n",
    "\\begin{bmatrix}\n",
    "y^{(1)}\\\\\n",
    "y^{(2)}\\\\\n",
    "y^{(3)}\\\\\n",
    "... \\\\\n",
    "y^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theta vector will have as many lines as there are parameters +1 (for the constant). \n",
    "$$ \\theta =\n",
    "\\begin{bmatrix}\n",
    "a\\\\\n",
    "b\\\\\n",
    "c\\\\\n",
    "... \\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $X$ initially looks like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}\\\\\n",
    "x^{(2)}\\\\\n",
    "x^{(3)}\\\\\n",
    "x^{(m)}\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to add a degree to the polynomial, it adds a feature to our $X$. And this feature will contain $x^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of polynomial of degree 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1, x^{(1)2}_2\\\\\n",
    "x^{(2)}_1, x^{(2)2}_2\\\\\n",
    "x^{(3)}_1, x^{(3)2}_2\\\\\n",
    "\\dots, \\dots\\\\\n",
    "x^{(m)}_1,x^{(m)k}_2\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of polynomial of degree 3: (In this case the third feature will be of power 3.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1, x^{(1)2}_2, x^{(1)3}_3\\\\\n",
    "x^{(2)}_1, x^{(2)2}_2, x^{(2)3}_3\\\\\n",
    "x^{(3)}_1, x^{(3)2}_2, x^{(3)3}_3\\\\\n",
    "\\dots, \\dots,\\dots \\\\\n",
    "x^{(m)}_1,x^{(m)2}_2, x^{(m)3}_3\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And so on and so forth. Of course, don't forget at the end to add a feature with only 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_1, x^{(1)2}_2, ..., x^{(m)k}_{n}, 1\\\\\n",
    "x^{(2)}_1, x^{(2)2}_2, ..., x^{(m)k}_{n}, 1\\\\\n",
    "x^{(3)}_1, x^{(3)2}_2, ..., x^{(m)k}_{n}, 1\\\\\n",
    "x^{(m)}_1,x^{(m)k}_2, ..., x^{(m)k}_{n}, 1\\\\\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a matrix `X` for a 3-degree polynomial $[x, x^2, x^3, 1]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones(X.shape)\n",
    "X = np.hstack((X, X**2, X**3, ones))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Initialize the random `theta` vector, with 4 elements (because `X` has four columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.rand(4, 1).reshape(-1, 1)\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create the `model`. It is always the same:\n",
    "$$Y = X \\cdot \\theta $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, theta):\n",
    "    return X.dot(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `MSE` function. It is always the same too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X, theta)\n",
    "    return 1/(2*m) * np.sum((y_pred - y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = MSE(X, y, theta)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `grad` function. Again, it is always the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(X, y, theta):\n",
    "    m = len(y)\n",
    "    y_pred = model(X, theta)\n",
    "    return 1/m * X.T.dot(y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "Again...\n",
    "1. Create a `gradient_descent` function that receives as parameter `X`, `y`, `theta`, `learning_rate` and `n_iterations`.\n",
    "2. In the function, create a variable `cost_history` with a matrix filled with 0 and which has a length of `n_iterations`. We will use it to display the histogram of the model learning process.\n",
    "3. Create a loop that iterates up to `n_iterations`.\n",
    "4. In the loop, update `theta` with the formula of the gradient descent (the example above).\n",
    "5. In the loop, update `cost_history[i]` with the values of `MSE(X,y,theta)`.\n",
    "6. Return `theta` and `cost_history`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, learning_rate, n_iterations):\n",
    "    \n",
    "    cost_history = np.zeros(n_iterations)\n",
    "\n",
    "    for i in range(0, n_iterations):\n",
    "        theta = theta - learning_rate * grad(X, y, theta)\n",
    "        cost_history[i] = cost_function(X, y, theta)\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `n_iterations` and `learning_rate`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iterations = 1000\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create variables `theta_final`, `cost_history` and call `gradient_descent()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final, cost_history = gradient_descent(X, y, theta, learning_rate, n_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Create a `predictions` variable that contains `model(X, theta_final)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model(X, theta_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Display your `predictions` and the true values of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], y)\n",
    "plt.scatter(X[:,0], predictions, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It says it looks like this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../assets/poly3.JPG\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Plot `cost_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(cost_history)), cost_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coef_determination(y, pred):\n",
    "    u = ((y - pred)**2).sum()\n",
    "    v = ((y - y.mean())**2).sum()\n",
    "    return 1 - u/v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_determination(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try to improve your model by adding a degree to your polynomial model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, you must feel like this now: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://media.giphy.com/media/DHqth0hVQoIzS/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next? \n",
    "Linear models might look simple but they can get very complicated.\n",
    "You might look into **Ridge Regression** or **Lasso Regression** if you want to further deepen your knowledge.\n",
    "\n",
    "- [Statquest - Regularization explained (Lasso & Ridge)](https://youtu.be/Q81RR3yKn30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regression",
   "language": "python",
   "name": "regression"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
