{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning using pandas\n",
    "\n",
    "In this notebook, you'll see how clean structured data using `pandas`. We are aware that you have already done an extensive review of pandas, and are now experts in data manipulation, so we'll keep it short.\n",
    "\n",
    "We are going to use a dataset about New York City Squirrels taken from [NYC Open Data](https://opendata.cityofnewyork.us/) which is a great resource for data sets.\n",
    "This dataset is quite small and this notebook will just be an overview of different stuff to look out for when cleaning data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"../../../datasets/NYC Squirrels/data.csv\"\n",
    "df = pd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things to check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values\n",
    "\n",
    "There are different ways that a value can be missing. It can be just an empty string (`\"\"`), or `NaN` (Not a Number).\n",
    "`np.nan`, `None` and `NaT` (for `datetime64` types) are standard missing value for Pandas.\n",
    "\n",
    "You can either delete the rows/attribute if there are too many missing, or replace them. I can't give you a certain threshold to delete or replace, it really depends on the variable, on the context, and what you are going to do with the data later on.\n",
    "In the same vein, there are many different ways to replace a value, and it will depend on the type of data you have. [[More info here]](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html)\n",
    "\n",
    "Pandas provides `isnull()`, `isna()` functions to detect missing values. Both of them do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are {} rows of data\".format(len(df)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we have 3023 rows of data. Looking at the number of NaN values per column, one can confidently say that columns where we have > 1000 missing values are not worth keeping. The case can be made that, for certain scenarios, the columns `Highlight Fur Color` should be kept too, but here we are going to drop it because the column `Combination of Primary and Highlight Color` combines both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is going to feature small code blocks that you'll have to complete.\n",
    "\n",
    "The first task is to count how many nan values there are per attribute. Hint: use `df.isnull()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many rows of each attribute are NaN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've identified all the columns that have NaN values, we would like to drop those with more than 1000 NaN values.\n",
    "\n",
    "Can you do that? Hint: `df.drop()`\n",
    "  \n",
    "Moreover, can you print the columns that were dropped, along with the number of missing values that they had?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns that have more than 1000 missing values\n",
    "\n",
    "\n",
    "        \n",
    "# Print the columns that were dropped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute type\n",
    "\n",
    "Sometimes, an attribute does not have the correct type, e.g. integers are labeled as floats. So it is useful to check if it needs to be changed or not.\n",
    "\n",
    "\n",
    "Please print the dtypes of every attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dtype of every attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, for `Hectare`, one could think that the variable is just the size of the plot of land. However, it seems it is just an identifier for which plot of land it is. One would have to see the documentation to know for sure. In this case, the `object` dtype is appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print unique values of the \"Hectare\" attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attribute values\n",
    "\n",
    "Carefully check each attribute to see if the unique values are coherent and make sense.\n",
    "Moreover, check that minimum and maximum values make sense. (e.g. age of a person that is negative)\n",
    "\n",
    "Hint: use your previously acquired knowledge of extracting unique values.\n",
    "Be careful to limit the attributes that you print by the number of unique values they have, as there are fields that have all their values that are unique, and we don't want a kilometer long list of each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the number of unique values of each column, only printing those that have less than 100 unique values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\").T  # Transpose the data frame so that it fits in a cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Does something stand out to you?**\n",
    "\n",
    "In the column `Combination of Primary and Highlight Color` for example, we have a value that is only `\"+\"`.\n",
    "Again, there are multiple ways to deal with this. When you can't figure out what the value means (like in this case), you can drop it. Sometimes, when you are able to figure out which one is meant, you can replace it with the correct value. This is where the domain knowledge come into play, knowing which variable can be replaced with what, and what it could mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example, from another dataset containing vehicle types where the vehicle type was manually encoded.\n",
    "You see how `\"Tow truck\"` appears in many different forms.\n",
    "The right approach is to consolidate all of the different values of tow truck into a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trucks = [\"Pick-up Truck\", \"Tractor Truck Diesel\", \"Box Truck\", \"Tractor Truck Gasoline\", \"Tow Truck / Wrecker\", \"Beverage Truck\", \"truck\", \"Armored Truck\", \"TRUCK\", \"Truck\", \"Tow Truck\", \"FIRE TRUCK\", \"PICK-UP TRUCK\", \"BOX TRUCK\", \"Fire Truck\", \"box truck\", \"TRUCK FLAT\", \"FIRTRUCK\", \"TRUCK VAN\", \"FIRETRUCK\", \"FDNY TRUCK\", \"dump truck\", \"Fire truck\", \"UPS TRUCK\", \"fire truck\", \"FOOD TRUCK\", \"MAIL TRUCK\", \"USPS TRUCK\", \"tow truck\", \"TOW TRUCK\", \"Tow truck\"]\n",
    "[truck for truck in trucks if \"tow\" in truck.lower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duplicates\n",
    "\n",
    "Search for duplicates and remove them\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are no duplicates\n",
    "df.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there had been, call `df.drop_duplicates(inplace=True)` to remove them.\n",
    "\n",
    "Again, this is just a short introduction to data cleaning of structured data, there are many more steps that one could optionally take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and reading material\n",
    "\n",
    "[Data cleaning challenge - Kaggle](https://www.kaggle.com/rtatman/data-cleaning-challenge-handling-missing-values)  \n",
    "[Preparing data for a machine learning model - Dataquest](https://www.dataquest.io/blog/machine-learning-preparing-data/)  \n",
    "[Preliminary data cleaning - Kaggle](https://www.kaggle.com/quannguyen135/preliminary-data-cleaning-with-python)  \n",
    "[Pandas data cleaning cheatsheet - Elite Data Science](https://elitedatascience.com/python-cheat-sheet)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
