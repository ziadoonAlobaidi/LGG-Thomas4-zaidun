{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combat overfitting and prevent biasing your model\n",
    "\n",
    "In this notebook and in the next ones, you'll see different ways to combat overfitting.\n",
    "\n",
    "In machine learning, what we want our model to do is predict the class of data that the model has never seen, we want it to **generalize** to unseen data. We do not want it to memorize training data.\n",
    "\n",
    "Thus, overfitting occurs when the model does just that, memorize data, and when it can not correctly classify data that it has never seen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "\n",
    "Like you've seen in the previous notebook, you can simulate *unseen* data by simply using a train-test split and verifying the accuracy on the test set.\n",
    "This is usually done when you have a lot of data and you can easily spare a bit of data to constitute a testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=41, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating, fitting the data to the classifier (training)\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Evaluation classifier on training set and testing set\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(\"Evaluating the model on the training set yields an accuracy of {}%\".format(score*100))\n",
    "score=classifier.score(X_test, y_test)\n",
    "print(\"Evaluating the model on the testing set yields an accuracy of {:.2f}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, you can see that when changing the random_state, which changes how the different samples are distributed between the training and the testing set, the testing accuracy changes.\n",
    "Indeed, the elements in the training set and in the testing set are different, so the model learns different things and it gets evaluated on different samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the testing set yields an accuracy of 93.57% with random state 0\n",
      "Evaluating the model on the testing set yields an accuracy of 92.98% with random state 1\n",
      "Evaluating the model on the testing set yields an accuracy of 92.40% with random state 2\n",
      "Evaluating the model on the testing set yields an accuracy of 94.74% with random state 3\n"
     ]
    }
   ],
   "source": [
    "# Repeat the actions above for different random states\n",
    "for random_state in range(4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=random_state, test_size=0.3)\n",
    "    classifier = DecisionTreeClassifier(random_state=1)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    score=classifier.score(X_test, y_test)\n",
    "    print(\"Evaluating the model on the testing set yields an accuracy of {:.2f}% with random state {}\".format(score*100, random_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A difference of 2% is not negligible, especially when you are trying to create state of the art (SOTA) models, which are models that are the best in their category. A popular competition where many research teams around the world try to create the best model is, for example, [ILSVRC](http://image-net.org/challenges/LSVRC/), which is based around the ImageNet dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "When you don't have a lot of data, but you still want to test a model, you can use cross-validation.\n",
    "A way to get a single value that can accurately represent the performance of the model is to take the average over many different train-test-splits.\n",
    "This is called *k-fold cross-validation*, which uses `k` different train-test splits.\n",
    "Indeed, this splits the data set into k differents groups, called splits.\n",
    "Then, it will train on (k-1) splits and test on 1 of the splits. This operation gets repeated k times so that each split is tested once, while being trained on the other splits.\n",
    "\n",
    "<img src=\"assets/crossvalidation.png\" />\n",
    "\n",
    "[[Image source]](https://www.researchgate.net/publication/331209203_Tectonic_discrimination_of_olivine_in_basalt_using_data_mining_techniques_based_on_major_elements_a_comparative_study_from_multiple_perspectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90350877 0.90350877 0.92105263 0.94736842 0.91150442]\n",
      "Accuracy: 91.74% (+/- 1.63)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "scores = cross_val_score(classifier, X, y, cv=5) # cv is the number of folds (k)\n",
    "print(scores)\n",
    "\n",
    "# It is always a good practice to show the mean AND the standard deviation of the model accuracy\n",
    "print(\"Accuracy: {:.2f}% (+/- {:.2f})\".format(scores.mean() * 100, scores.std() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find many different cross validation strategies in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and additional reading material\n",
    "<a id='references'></a>\n",
    "\n",
    "[Difference between test and validation set - Machine Learning Mastery](https://machinelearningmastery.com/difference-test-validation-datasets/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modeleval] *",
   "language": "python",
   "name": "conda-env-modeleval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
