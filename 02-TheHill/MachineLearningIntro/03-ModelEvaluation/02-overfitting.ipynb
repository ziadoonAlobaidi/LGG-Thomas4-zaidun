{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting\n",
    "\n",
    "In this notebook, we are going to tackle a big problem in the field of machine learning : overfitting.\n",
    "\n",
    "*The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then* **over-fitting occurs when a model begins to \"memorize\" training data rather than \"learning\" to generalize from a trend.**\n",
    "\n",
    "\n",
    "\n",
    "For this, we're going to use the popular Iris dataset, which is a dataset of flowers, of which `sklearn` provides easy access to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data set\n",
    "dataset = datasets.load_iris()\n",
    "X, y = dataset.data, dataset.target\n",
    "\n",
    "# Here you see that there are 3 different classes\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To show a big case of overfitting, we're going to take all the samples or class 0 and 1, and only\n",
    "# a single sample from class 2\n",
    "\n",
    "limit = 101\n",
    "X_train, X_test = X[:limit], X[limit:]\n",
    "y_train, y_test = y[:limit], y[limit:]\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape :  (101, 4)\n",
      "X_test shape :  (49, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train shape : \", X_train.shape)\n",
    "print(\"X_test shape : \", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(random_state=1)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this example, we're going to use a simple DecisionTree to classify the 3 classes.\n",
    "# Here, we initialize the model and we fit the training data onto it\n",
    "classifier = DecisionTreeClassifier(random_state=1)\n",
    "classifier.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the training set yields an accuracy of 100.0%\n",
      "Evaluating the model on the testing set yields an accuracy of 48.98%\n"
     ]
    }
   ],
   "source": [
    "score = classifier.score(X_train, y_train)\n",
    "print(\"Evaluating the model on the training set yields an accuracy of {}%\".format(score*100))\n",
    "score=classifier.score(X_test, y_test)\n",
    "print(\"Evaluating the model on the testing set yields an accuracy of {:.2f}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see here ?\n",
    "\n",
    "We have trained the model on the training set, which, as a reminder, only contains classes 0 and 1 and a single example of class 2.\n",
    "\n",
    "Testing the model on the training set yields perfect accuracy. Indeed, the model has already seen the samples and the whole model was build to increase the training accuracy. So it is indeed expected that the accuracy on the training set is pretty high.\n",
    "\n",
    "However, when we're looking at the testing set, which contains only samples of class 2, the accuracy is not that great (at all). How is that ?\n",
    "Well, the problem is that the model has only seen a single example of class 2, and thus it relied too much on that single sample to decide whether the test samples were of class 2 or not. It **overfitted** the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/overfitting.png\" />\n",
    "\n",
    "[[Image source]](https://www.educative.io/edpresso/overfitting-and-underfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combat this, we can shuffle the training and testing sets so that they have a random number of samples of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=1, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating the model on the training set yields an accuracy of 100.0%\n",
      "Evaluating the model on the testing set yields an accuracy of 96.67%\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X_train, y_train)\n",
    "score = classifier.score(X_train, y_train)\n",
    "print(\"Evaluating the model on the training set yields an accuracy of {}%\".format(score*100))\n",
    "score=classifier.score(X_test, y_test)\n",
    "print(\"Evaluating the model on the testing set yields an accuracy of {:.2f}%\".format(score*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the accuracy on the testing set rose to be closer to the training set accuracy ?\n",
    "\n",
    "In the next chapter, you'll see other ways how overfitting can appear, and where you'll need to be careful when manipulating your data.\n",
    "Moreover, you'll see many ways to prevent your model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and additional material\n",
    "\n",
    "[Overfitting in machine learning - Elite Data Science](https://elitedatascience.com/overfitting-in-machine-learning)\n",
    "\n",
    "[Overfitting and underfitting - Machine Learning Mastery](https://machinelearningmastery.com/overfitting-and-underfitting-with-machine-learning-algorithms/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
