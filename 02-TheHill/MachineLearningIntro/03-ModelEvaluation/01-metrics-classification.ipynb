{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Evaluating your machine learning model is an important step.\n",
    "Some see it as nothing more than looking at the accuracy, and then you're done.\n",
    "However, it is a bit more complicated than that (who would have thought??)\n",
    "\n",
    "In this notebook, you're going to see different metrics and evaluation strategies.\n",
    "\n",
    "## Accuracy\n",
    "\n",
    "Accuracy is a pretty simple metric.\n",
    "It is the ratio of the number of correct predictions to the total number of input samples.\n",
    "\n",
    "$$ accuracy = \\frac{nb.\\ correct\\ predictions}{nb.\\ input\\ samples}$$\n",
    "\n",
    "Assuming a binary classification problem (it also works with more classes),\n",
    "using accuracy works pretty well when there is an equal number of samples of each class in the training set.\n",
    "However, when there is an imbalance in the samples, for example there are 95 samples of class *A*\n",
    "and 5 samples of class *B*, for a total of 100, the model can easily get **at least** 95\\% accuracy.\n",
    "It can just to that by always predicting class *A*, leading you to believe that the\n",
    "model is doing great, even though it miss-classified all the samples of class *B* (5 samples).\n",
    "This is very problematic in fields such as medicine, when trying to find a rare disease for example.\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "To get a finer granularity in the predictions, one can use a confusion matrix.\n",
    "\n",
    "Let's again assume the we have a binary classification problem, and we can predict class *A* or class *B*.\n",
    "Class *A* is (arbitrarily) labeled as the positive class, and thus class *B* as the negative class.\n",
    "\n",
    "There are 4 possible cases:\n",
    "\n",
    "- The true output of a sample is A, we predict A --> True Positive (TP)\n",
    "- The true output of a sample is B, we predict B --> True Negative (TN)\n",
    "- The true output of a sample is B, we predict A --> False Positive (FP)\n",
    "- The true output of a sample is A, we predict B --> False Negative (FN)\n",
    "\n",
    "This gives us a better of what went wrong in the model. Maybe we find that the model always predicts the positive class,\n",
    "and so we could maybe give the model more negative classes as input to balance it out. A lot of information can be\n",
    "gathered from a simple confusion matrix.\n",
    "\n",
    "![Confusion Matrix (Image)](./assets/confusion_matrix.png) ![Confusion Matrix Values (Image)](./assets/confusion_matrix_values.png)\n",
    "\n",
    "In this data set, we have 100 samples, of which 51 (48 + 3) are positive, and 49 (42 + 7) are negative.\n",
    "One can still get the accuracy by summing up the True Positives and the True Negatives, and dividing by the total number of samples.\n",
    "We thus get $\\frac{48 + 42}{100} = 90\\%$.\n",
    "\n",
    "## ROC Curve (Receiver Operating Characteristic Curve)\n",
    "\n",
    "A ROC curve is a graph showing the performance of a classification model at different classification thresholds.\n",
    "\n",
    "**What is a classification threshold?**\n",
    "\n",
    "Many machine learning algorithms are able to output probabilities of belonging to a given class instead of strict class labels.\n",
    "This is useful because it can tell you the certainty or the uncertainty of the prediction.\n",
    "\n",
    "The classification threshold is the limit you set where all elements falling\n",
    "on one side of the threshold are labelled as one class and the elements landing on the opposite side are labeled as the other class (in binary classification).\n",
    "A common misconception is that the threshold should always be at 50\\%, but the threshold is problem dependent.\n",
    "Indeed, for a given problem, you might want to set the threshold to a different value. For example in spam classification,\n",
    "you'd prefer having more false negatives (spam interpreted as not spam (called ham)) than false positive (ham interpreted as spam), or the reverse.\n",
    "\n",
    "![Classification Threshold (Image)](./assets/classification_threshold.png)\n",
    "[Image source](https://www.researchgate.net/publication/327847657_One-class_Quantification)\n",
    "\n",
    "How to plot the ROC curve?\n",
    "It is pretty easy.\n",
    "\n",
    "For every classification threshold that you choose, you will plot a point of the True Positive Rate (TPR) against the\n",
    "False Positive Rate (FPR), where TPR is on y-axis and FPR is on the x-axis\n",
    "\n",
    "The *true positive rate*, also called *recall* or *sensitivity*, has the following equation (based on the terms coined in the confusion matrix section):\n",
    "\n",
    "**TPR/Recall/Sensitivity** $= \\frac{TP}{TP+FN}.$\n",
    "\n",
    "The *false positive rate* is the opposite of *specificity*. Be careful, *sensitivity* and *specificity* are similar in name, but different in meaning.\n",
    "We have\n",
    "\n",
    "**Specificity** = $\\frac{TN}{TN+FP}$\n",
    "\n",
    "and **FPR** = $1 - \\text{ specificity } = \\frac{FP}{TN+FP}$.\n",
    "\n",
    "You will rarely ever need to know the formulas by heart, this is just to give you a sense how all of this is connected.\n",
    "Finally, we have a graph of a ROC curve.\n",
    "The closer the graph is to the *upper left* corner, the better the model is at distinguishing between classes.\n",
    "In the graph below, you also see the *No Skill* line, which shows the skill of a random model, which cannot distinguish between classes.\n",
    "\n",
    "To get a numeric value for such a curve, which makes it easier to compare multiple models, one can use the **AUC score**, which is simply the area\n",
    "under the ROC curve.\n",
    "The **A**rea **U**nder **C**urve is one of the most popular evaluation metrics.\n",
    "*One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example* [[Source]](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc)\n",
    "\n",
    "![ROC Curve (Image)](assets/roc_curve.png)\n",
    "\n",
    "There are many more evaluation metrics, most notably **precision**, **recall** (which we've seen above), which can be graphed as a **Precision-Recall (PR) Curve.**\n",
    "A popular one is the **F1-Score**, which combines precision and recall into one numerical value. Similar to this is also **Matthews Correlation Coefficient (MCC).\n",
    "**.\n",
    "I highly encourage you to read more about the topic and educate yourself further by looking at the additional reading material at the bottom of the notebook.\n",
    "\n",
    "## References and more reading material\n",
    "\n",
    "[A Gentle Introduction to Threshold-Moving for Imbalanced Classification - Machine learning mastery](https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/)\n",
    "\n",
    "[Classification Accuracy is Not Enough: More Performance Measures You Can Use - Machine learning mastery](https://machinelearningmastery.com/classification-accuracy-is-not-enough-more-performance-measures-you-can-use/)  \n",
    "\n",
    "[Understanding AUC - ROC Curve - Towards Data Science](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)\n",
    "\n",
    "[How to Use ROC Curves and Precision-Recall Curves for Classification in Python - Machine learning mastery](https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/)\n",
    "\n",
    "[Matthews Correlation Coefficient is The Best Classification Metric Youâ€™ve Never Heard Of - towardsdatascience](https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_last",
   "language": "python",
   "name": "env_last"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
