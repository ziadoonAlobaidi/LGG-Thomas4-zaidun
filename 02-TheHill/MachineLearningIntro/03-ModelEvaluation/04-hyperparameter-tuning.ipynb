{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the best hyper-parameters\n",
    "\n",
    "Most of the machine learning algorithms, like `DecisionTreeClassifier`, `RandomForestClassifier`, and so on..., have parameters that you can tune to change the behaviour of the model.\n",
    "\n",
    "These are called *hyper-parameters*. The difference between *parameters* and *hyper-parameters* in machine learning is that the former are the properties of the training data that are learned during training, while the latter are set before training and do not change.\n",
    "\n",
    "For example, for `DecisionTreeClassifier`, the hyper-parameters are \n",
    "\n",
    "- the splitting criterion\n",
    "- the maximum tree depth\n",
    "- ...\n",
    "\n",
    "You'll find more info about all the hyper-parameters of `DecisionTreeClassifier`  in the [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html).\n",
    "\n",
    "Trying to find the best hyper-parameters is called *hyper-parameter tuning*.\n",
    "However, you have to pay attention so as to not bias the model with data.\n",
    "Indeed, you can't rely on the accuracy of the testing set to choose the hyper-parameters, as that would mean that you chose the parameters based on the testing set, thus using that data in the training procedure.\n",
    "The testing set shall be the LAST thing you evaluate on, and it shall remain unseen until the very end.\n",
    "\n",
    "What you do is you split your training again to get a validation set, which you will use to choose the best hyper-parameters.\n",
    "There is a lot of conflicting information as to how you call the set that you evaluate last. Some call it the testing set, and others the validation set. In the world of academia, the term for the set of data that you use last is the testing set.\n",
    "You'll thus have a training-validation-test split. A rule of thumb is for this split to be 50-30-20, but this can vary with the quantity of data that you have at your disposition.\n",
    "\n",
    "To perform hyper-parameter tuning using `scikit-learn`, there is an useful method called `GridSearchCV`, which explores every combination of the parameters you specify and does cross-validation to choose the best ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating train-test split and classifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=41, test_size=0.2)\n",
    "decision_tree = DecisionTreeClassifier(random_state=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "max_features : [0.1, 0.2, 0.30000000000000004, 0.4, 0.5, 0.6, 0.7000000000000001, 0.8, 0.9]\n",
      "max_depth : [1, 2, 4, 8, None]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting all the parameters we want to test\n",
    "params = {\n",
    "    'max_features' : np.arange(0.1,1,0.1).tolist(), #Number of features to consider as a fraction of all features\n",
    "    'max_depth': [1,2,4,8, None] # Depth of the tree\n",
    "}\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for k,v in params.items():\n",
    "    print(\"{} : {}\".format(k,v))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=-1)]: Done 210 out of 225 | elapsed:    0.9s remaining:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 225 out of 225 | elapsed:    0.9s finished\n"
     ]
    }
   ],
   "source": [
    "# Setting up the grid search that will test every combination of parameters\n",
    "gridsearch = GridSearchCV(estimator = decision_tree,\n",
    "                        param_grid = params,\n",
    "                        scoring = 'accuracy', \n",
    "                        cv = 5, # Use 5 folds\n",
    "                        verbose = 1,\n",
    "                        n_jobs = -1 #Use all but one CPU core\n",
    "                        )\n",
    "\n",
    "# As we are doing cross-validation on the training set, the testing set X_test is untouched\n",
    "result = gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are : {'max_depth': 4, 'max_features': 0.6}\n",
      "The best accuracy is 94.51%:\n",
      "The generalization accuracy of the model is 92.11%\n"
     ]
    }
   ],
   "source": [
    "print(\"The best parameters are :\", result.best_params_)\n",
    "print(\"The best accuracy is {:.2f}%:\".format(result.best_score_ * 100))\n",
    "\n",
    "# We can now use the testing set with the optimal hyper-parameters to get the final generalization accuracy\n",
    "decision_tree = result.best_estimator_\n",
    "score = decision_tree.score(X_test, y_test)\n",
    "print(\"The generalization accuracy of the model is {:.2f}%\".format(score * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, you would do all of the above multiple times by choosing different train-test splits, and averaging the score.\n",
    "If we transform the above into a method that we can call, we can easily run this multiple times.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_grid_search(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Performs a grid search using the training set given.\n",
    "    \"\"\"\n",
    "    # Setting all the parameters we want to test\n",
    "    params = {\n",
    "        'max_features' : np.arange(0.1,1,0.1).tolist(), #Number of features to consider as a fraction of all features\n",
    "        'max_depth': [1,2,4,8, None] # Depth of the tree\n",
    "    }\n",
    "\n",
    "    gridsearch = GridSearchCV(estimator = decision_tree,\n",
    "                            param_grid = params,\n",
    "                            scoring = 'accuracy', \n",
    "                            cv = 5, # Use 5 folds\n",
    "                            verbose = 0,\n",
    "                            n_jobs = -1 #Use all but one CPU core\n",
    "                            )\n",
    "\n",
    "    # As we are doing cross-validation on the training set, the testing set X_test is untouched    \n",
    "    return gridsearch.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generalization accuracy of the model is 92.11%\n"
     ]
    }
   ],
   "source": [
    "# Redoing the same computation as before, but this time\n",
    "# using the method we created to show that we have the same results\n",
    "result = single_grid_search(X_train, y_train)\n",
    "decision_tree = result.best_estimator_\n",
    "score = decision_tree.score(X_test, y_test)\n",
    "print(\"The generalization accuracy of the model is {:.2f}%\".format(score * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Split 1: Accuracy is 92.11% ###\n",
      "### Split 2: Accuracy is 92.98% ###\n",
      "### Split 3: Accuracy is 90.35% ###\n",
      "### Split 4: Accuracy is 87.72% ###\n",
      "### Split 5: Accuracy is 96.46% ###\n",
      "The mean generalization accuracy of the model is 91.92% (+/- 2.89%)\n"
     ]
    }
   ],
   "source": [
    "# Now we can create k train-test splits using KFold\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Using KFold instead of calling multiple times train_test_split to ensure that each\n",
    "# sample goes into a single split only\n",
    "kf = KFold(n_splits=5, random_state=45, shuffle=True)\n",
    "\n",
    "split = 0\n",
    "scores = []\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    result = single_grid_search(X_train, y_train)\n",
    "    \n",
    "    decision_tree = result.best_estimator_\n",
    "    score = decision_tree.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(\"### Split {}: Accuracy is {:.2f}% ###\".format(split := split + 1, score*100))\n",
    "    \n",
    "print(\"The mean generalization accuracy of the model is {:.2f}% (+/- {:.2f}%)\".format(np.mean(scores) * 100, np.std(scores) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and additional reading material\n",
    "\n",
    "[GridSearchCV - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "[KFold - scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:modeleval] *",
   "language": "python",
   "name": "conda-env-modeleval-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
