{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concurrency and Parallelism in Python\n",
    "Until now, we have used Python sequentially: instructions were executed one after the other.\n",
    "\n",
    "However, Python offers in its standard library several modules for **concurrent** and **parallel** execution: several instructions of code will execute at the same time, or seem to.\n",
    "\n",
    "In Python 3, there are 3 main ways to execute code in concurrently on your local machine:\n",
    "\n",
    "|Concurrency Type | Switching Decision | Number of CPU cores|\n",
    "| --- | --- | --- |\n",
    "|Pre-emptive multitasking (threading) | The operating system decides when to switch tasks external to Python. | 1|\n",
    "|Cooperative multitasking (asyncio) | The tasks decide when to give up control. | 1|\n",
    "|Multiprocessing (multiprocessing) | The processes all run at the same time on different processors. | Many| \n",
    "\n",
    "In the scope of this course, we will not cover cooperative multitasking. Most of your needs can be covered by simple threading or multiprocessing but that should not discourage you from learning about `asyncio` as it is pretty neat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple threading example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we define a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "def task(id):\n",
    "    print(f\"Task {id}: starting. \")\n",
    "    sleep(.1) # .1 seconds\n",
    "    print(f\"Task {id}: finishing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we want to call it 5 times. We measure how long it will take to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. \n",
      "Task 0: finishing. \n",
      "Task 1: starting. \n",
      "Task 1: finishing. \n",
      "Task 2: starting. \n",
      "Task 2: finishing. \n",
      "Task 3: starting. \n",
      "Task 3: finishing. \n",
      "Task 4: starting. \n",
      "Task 4: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.5237274999999997 seconds.\n"
     ]
    }
   ],
   "source": [
    "from time import perf_counter\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "for i in range(5):\n",
    "    task(i)\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wait 5 times for .1 second, so .5 seconds. What if we start 5 times the `task()` method at once?\n",
    "\n",
    "We use the ``threading`` module. We create a ``Thread`` object whose contructor takes the function and its arguments as parameters. Then, we ``start()`` the thread.\n",
    "\n",
    "This executes the tasks *concurrently* on one CPU core. Picture the OS passing \"the ball\" to each thread one by one and waiting for them to send it back in their own time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. Task 1: starting. \n",
      "\n",
      "Task 2: starting. \n",
      "Task 3: starting. \n",
      "Task 4: starting. \n",
      "Task 0: finishing. Task 1: finishing. Task 3: finishing. \n",
      "\n",
      "\n",
      "Task 4: finishing. Task 2: finishing. \n",
      "\n",
      "\n",
      "Time spent inside the loop: 0.10274862500000026 seconds.\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "threads = list()\n",
    "for i in range(5):\n",
    "    thread = Thread(target=task, args=(i,)) # New thread will run \"task\" with argument \"i\"\n",
    "    threads.append(thread) # To keep track of all the treads\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:  # The second loop is necessary. start() everything then join() everything.\n",
    "    thread.join() # Make sure all the threads are done before continuing\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what we observe:\n",
    "\n",
    "* **Evidence of concurrent execution:** \n",
    "  * The final **print()** function executes after only .1 second.\n",
    "* **Threading does not guaranty order:** \n",
    "  * The threads do *not* finish in the order they were started.\n",
    "* **Evidence of race conditions:** \n",
    "  * The outputs are mangled. \n",
    "  * Observe how some outputs are on the same line, showing a thread started to write while another was still writing.\n",
    "  * The threads are fighting for the same resource. In this case, the standard output, your screen.\n",
    "\n",
    "### Locks avoid race conditions\n",
    "\n",
    "A simple way to \"synchronize\" our threads, to share resources gracefully, we use a *lock*:\n",
    "\n",
    "When accessing a shared resource, we tell the lock to block its access until released. If another thread wants to use this resource, it must wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from threading import RLock\n",
    "\n",
    "rlock = RLock() # Needs to be outside the function. Created once, used by every thread.\n",
    "\n",
    "def task_locked(i):\n",
    "    with rlock: # Equivalent of rlock.acquire() [insert code] rlock.release()\n",
    "        task(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. \n",
      "Task 0: finishing. \n",
      "Task 1: starting. \n",
      "Task 1: finishing. \n",
      "Task 2: starting. \n",
      "Task 2: finishing. \n",
      "Task 3: starting. \n",
      "Task 3: finishing. \n",
      "Task 4: starting. \n",
      "Task 4: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.5225427499999995 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "threads = list()\n",
    "for i in range(5):\n",
    "    thread = Thread(target=task_locked, args=(i,))\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the lock is blocking the whole ``task()`` function. It is better to lock at a more granular level to free up the locked resources more often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_locked(id):\n",
    "    with rlock:\n",
    "        print(f\"Task {id}: starting. \")\n",
    "    sleep(.1) # This is the part that takes time and should be run concurrently\n",
    "    with rlock:\n",
    "        print(f\"Task {id}: finishing. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. \n",
      "Task 1: starting. \n",
      "Task 2: starting. \n",
      "Task 3: starting. \n",
      "Task 4: starting. \n",
      "Task 1: finishing. \n",
      "Task 0: finishing. \n",
      "Task 3: finishing. \n",
      "Task 2: finishing. \n",
      "Task 4: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.10432283299999945 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "threads = list()\n",
    "for i in range(5):\n",
    "    thread = Thread(target=task_locked, args=(i,))\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "    \n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOP equivalent\n",
    "\n",
    "To run the code concurrently, the ``Sleeper`` class inherits from the `Thread` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sleeper(Thread): # Sleeper is a Thread\n",
    "    def __init__(self, id): # Override Thread constructor\n",
    "        super().__init__() # Call the parent constructor. Not optional because we are overriding __init__\n",
    "        self.id = id\n",
    "\n",
    "    def run(self): # Called \"run\" to comply with the Thread module default behavior.\n",
    "        print(f\"Task {self.id}: starting. \")\n",
    "        sleep(.1)\n",
    "        print(f\"Task {self.id}: finishing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the ``run()`` method directly, we execute the code sequentially, as if ``Sleeper`` were not a ``Thread``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. \n",
      "Task 0: finishing. \n",
      "Task 1: starting. \n",
      "Task 1: finishing. \n",
      "Task 2: starting. \n",
      "Task 2: finishing. \n",
      "Task 3: starting. \n",
      "Task 3: finishing. \n",
      "Task 4: starting. \n",
      "Task 4: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.5177888330000009 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "for i in range(5):\n",
    "    Sleeper(i).run() # Not using threads\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to call the ``start()`` method inherited from ``Thread`` (see [https://docs.python.org/2/library/threading.html#thread-objects](https://docs.python.org/2/library/threading.html#thread-objects))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. Task 1: starting. \n",
      "\n",
      "Task 2: starting. \n",
      "Task 3: starting. \n",
      "Task 4: starting. \n",
      "Task 1: finishing. \n",
      "Task 0: finishing. \n",
      "Task 3: finishing. Task 4: finishing. \n",
      "\n",
      "Task 2: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.11432316699999845 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "sleepers = list()\n",
    "for i in range(5):\n",
    "    sleepers.append(Sleeper(i))\n",
    "\n",
    "for sleeper in sleepers:\n",
    "    sleeper.start()\n",
    "\n",
    "for sleeper in sleepers:\n",
    "    sleeper.join()\n",
    "    \n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Locks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SleeperLock(Sleeper): # SleeperLock is a Sleeper and thus a Thread\n",
    "    rlock = RLock() # This is a class variable. It is created only once.\n",
    "\n",
    "    def run(self): # Override the parent run() method\n",
    "        with self.rlock: \n",
    "            print(f\"Task {self.id}: starting. \")\n",
    "        sleep(.1)\n",
    "        with self.rlock: \n",
    "            print(f\"Task {self.id}: finishing. \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 0: starting. \n",
      "Task 1: starting. \n",
      "Task 2: starting. \n",
      "Task 3: starting. \n",
      "Task 4: starting. \n",
      "Task 1: finishing. \n",
      "Task 2: finishing. \n",
      "Task 3: finishing. \n",
      "Task 4: finishing. \n",
      "Task 0: finishing. \n",
      "\n",
      "Time spent inside the loop: 0.10353191700000153 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "sleepers = list()\n",
    "for i in range(5):\n",
    "    sleepers.append(SleeperLock(i))\n",
    "\n",
    "for sleeper in sleepers:\n",
    "    sleeper.start()\n",
    "\n",
    "for sleeper in sleepers:\n",
    "    sleeper.join()\n",
    "    \n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processes vs Threads, the GIL\n",
    "\n",
    "For performance reasons, and because of its age, Python uses a Global Interpreter Lock or GIL ([https://realpython.com/python-gil/](https://realpython.com/python-gil/)). This *lock* enforces the use of a single CPU core for pre-emptive and cooperative concurrency. If we want to leverage modern CPUs and their many cores, we need to turn to *multiprocessing* to run processes in *parallel*.\n",
    "\n",
    "**How to decide? It depends on your problem. Is it:**\n",
    "\n",
    "* I/O bound: use threading. The bottleneck is not your CPU, so running on more than one core is not going to improve performance.\n",
    "* CPU bound: use multiprocessing. If the problem maxes out one CPU core, chances are spreading the load is the solution.\n",
    "\n",
    "**Why not use multiprocessing all the time then?**\n",
    "\n",
    "Because it creates a lot of overhead by *spawning* or *forking* (see below) entire copies of your main python process whereas threads are much more lightweight and can share ressources between them in addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we start with the sequential baseline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 4.535982041000002 seconds.\n"
     ]
    }
   ],
   "source": [
    "from math import factorial\n",
    "\n",
    "it_nb = 30 # Change this to ajust compute time on your machine\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "for i in range(it_nb):\n",
    "    factorial(10**5) # Big number\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, just to be sure, using threading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 4.470772624999999 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "threads = list()\n",
    "\n",
    "for i in range(it_nb):\n",
    "    thread = Thread(target=factorial, args=(10**5,))\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the problem could use more **power**, not faster **I/O**. Let's check multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 1.1059027090000022 seconds.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "processes = list()\n",
    "\n",
    "for i in range(it_nb):\n",
    "    process = Process(target=factorial, args=(10**5,))\n",
    "    processes.append(process)\n",
    "\n",
    "for process in processes:\n",
    "    process.start()\n",
    "\n",
    "for process in processes:\n",
    "    process.join()\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a significant speedup in compute time if your computer has more than one core. But not quite as fast as the number of your computer cores would suggest. Why? The overhead of creating entire processes. Also, we created ``it_nb`` processes. Do you have that amount of CPU cores?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores on my machine: 8\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "print(f\"Number of CPU cores on my machine: {cpu_count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map, Workers and Pools\n",
    "\n",
    "There are high-level tools to help you inject concurrency and parallelism into your code less painfully.\n",
    "\n",
    "One of the most handy tools is to think about your problem in relation to the ``map`` function ([https://realpython.com/python-map-function/](https://realpython.com/python-map-function/)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 4.548467917 seconds.\n"
     ]
    }
   ],
   "source": [
    "start_time = perf_counter()\n",
    "\n",
    "factorial_arguments = [10**5, ] * it_nb # [10**5, 10**5, ...]\n",
    "\n",
    "gen = map(factorial, factorial_arguments) # map is a generator (think yield)\n",
    "tuple(gen) # do the actual work\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the ``map`` version of the sequential code we saw above. Unsurprizingly, the execution time is similar.\n",
    "\n",
    "Let's see how to code it with threads. Even though we saw it would be useless in this CPU bound case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 4.487030957999998 seconds.\n"
     ]
    }
   ],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor \n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "with ThreadPoolExecutor() as pool: # Without arguments, ThreadPoolExecutor() will create as many workers as CPU cores + 4\n",
    "    tuple(pool.map(factorial, factorial_arguments))\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty neat and very limited code change. Note that ``ThreadPoolExecutor`` takes a ``max_workers`` argument so things don't get out of hand ([https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor))\n",
    "\n",
    "Now with multiprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Time spent inside the loop: 1.0267589169999987 seconds.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "start_time = perf_counter()\n",
    "\n",
    "with Pool() as pool: # Without arguments, Pool() will create as many workers as CPU cores (a good default).\n",
    "    tuple(pool.map(factorial, factorial_arguments))\n",
    "\n",
    "print(f\"\\nTime spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, minimal code change to add multiprocessing to your \"loop\".\n",
    "\n",
    "Note that all these ``map()`` functions have the added benefit of preserving order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 3, 6, 9, 1, 4, 7, 2, 5, 8, \n",
      "(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import get_context, RLock\n",
    "\n",
    "rlock = RLock()\n",
    "\n",
    "def task(id):\n",
    "    with rlock:\n",
    "        if id < 10: # Only to keep the output small\n",
    "            print(id, end=', ')\n",
    "    return id\n",
    "\n",
    "with get_context(\"fork\").Pool() as pool: # get_context(\"fork\"). Seems necessary on my M1 mac, otherwise the code hangs. Try the normal Pool() first. \n",
    "    res = tuple(pool.map(task, range(71))) # Any number. May require a few tries to show different outputs.\n",
    "\n",
    "print()\n",
    "print(res[:10]) # the first 10 results should be sufficient to show the 2 lists are different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are ordered even though the processes executed in a different order.\n",
    "\n",
    "**NB** for more explanations on *forks* and *spawns*:\n",
    "* [https://stackoverflow.com/questions/67999589/multiprocessing-with-pool-throws-error-on-m1-macbook](https://stackoverflow.com/questions/67999589/multiprocessing-with-pool-throws-error-on-m1-macbook)\n",
    "* [https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn](https://britishgeologicalsurvey.github.io/science/python-forking-vs-spawn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of the number of workers on performance for multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 workers] Time spent inside the loop: 4.791193207999999 seconds.\n",
      "[2 workers] Time spent inside the loop: 2.533712584 seconds.\n",
      "[4 workers] Time spent inside the loop: 1.3501607090000007 seconds.\n",
      "[8 workers] Time spent inside the loop: 1.047103208000003 seconds.\n",
      "[16 workers] Time spent inside the loop: 1.0464014579999983 seconds.\n",
      "[32 workers] Time spent inside the loop: 1.1825077920000027 seconds.\n",
      "[64 workers] Time spent inside the loop: 1.455161000000004 seconds.\n",
      "[128 workers] Time spent inside the loop: 1.9985152909999968 seconds.\n",
      "[256 workers] Time spent inside the loop: 3.2653933329999987 seconds.\n",
      "[512 workers] Time spent inside the loop: 6.078202250000004 seconds.\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "for i in range(10): \n",
    "    nb_workers = 2**i # Increase the number of workers exponentially\n",
    "\n",
    "    start_time = perf_counter()\n",
    "\n",
    "    with Pool(processes=nb_workers) as pool: # We fix the number of workers ourselves\n",
    "        tuple(pool.map(factorial, factorial_arguments))\n",
    "\n",
    "    print(f\"[{nb_workers} workers] Time spent inside the loop: {perf_counter() - start_time} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is probably good enough to leave it to Python to determine the number of workers by calling your pools without arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional resources\n",
    "\n",
    "* [https://fastapi.tiangolo.com/async/#asynchronous-code](https://fastapi.tiangolo.com/async/#asynchronous-code)\n",
    "* [https://realpython.com/python-concurrency/](https://realpython.com/python-concurrency/)\n",
    "* [https://realpython.com/async-io-python/](https://realpython.com/async-io-python/)\n",
    "* [https://realpython.com/python-gil/](https://realpython.com/python-gil/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
