{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic data collection on the Web"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start to tackle some nice web pages (HTML), we will discover the XML language which is a good introduction to scraping data on the web."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lists a few properties of the XML language.\n",
    "\n",
    "- XML was created to facilitate data exchange between machines and software.\n",
    "\n",
    "- XML is a language that is written using tags.\n",
    "\n",
    "- XML is a W3C recommendation, so it is a technology with strict rules to follow.\n",
    "\n",
    "- XML is intended to be understandable by everyone: people and machines alike.\n",
    "\n",
    "- XML allows us to create our own vocabulary using a set of customizable rules and tags.\n",
    "\n",
    "- XML is also compatible with the web so that data exchanges can be easily carried out over the Internet.\n",
    "\n",
    "- XML is therefore standardized, simple, but above all extensible and configurable so that any type of data can be described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of a XML document, which we have saved as `data.xml` in the `assets/` directory.\n",
    "\n",
    "Display its content!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./assets/data.xml\"\n",
    "file = open(filename, \"r\")\n",
    "print(file.read())\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line indicates the encoding (we always stay in the UTF-8 encoding). Then we notice that the \"users\" tag has other \"user\" tags that themselves have their own tags. The data is hierarchized in a tree and each node provides information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a small script that displays all the usernames.\n",
    "\n",
    "You will first have to install the `lxml` package. It depends on the `numpy` package, which will be installed alongside `lxml` if you use a standard package manager. However, some version of `numpy` give problems, so changing the version might be the first thing that you can troubleshoot if you fail to import `lxml`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "\n",
    "# I define my source document\n",
    "tree = etree.parse(filename)\n",
    "# I look at my document and identify the path to the tag to get to the \"user\" information\n",
    "\n",
    "# The user name we are looking for is in it's own tag, `name`. Which itself\n",
    "# is in a tag `user`, and lastly `user` is contained in a `users` tag.\n",
    "# So tree.xpath(\"/users/user/name\") contains the tags associated with our search\n",
    "for user in tree.xpath(\"/users/user/name\"):\n",
    "    # I only want to display the content (.text) of the `/users/user/name` tags\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.xpath(\"/users/user/name\")[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can display the attributes of the tags that store this information\n",
    "tree = etree.parse(filename)\n",
    "for user in tree.xpath(\"/users/user\"):\n",
    "    print(user.get(\"data-id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can refine the display by proposing to display only users whose job is Veterinary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = etree.parse(filename)\n",
    "# Quel joli petit dictionnaire\n",
    "for user in tree.xpath(\"/users/user[job='Veterinary']/name\"):\n",
    "    print(user.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web scraping\n",
    "\n",
    "Web scraping is a technique for automatically extracting information from websites. It is very useful in a lot of use cases:\n",
    "\n",
    "- **E-commerce**: create Excel sheet gathering all products from a provider's website\n",
    "- **Business prospecting**: extract contact information from websites (phone numbers, e-mail addresses...)\n",
    "- **Media analysis**: collect articles from online newspapers on a daily basis\n",
    "- ...\n",
    "\n",
    "Of course this task could be done by a human but this would be very painful and repetitive especially when the number of targeted pages is huge. To save time, we can automate it and, as usual, Python has a solution for that.\n",
    "\n",
    "The idea is to automate the human task by **fetching** pages and **extracting** data from them.\n",
    "\n",
    "**Fetching**\n",
    "\n",
    "Fetching is the downloading of the `html` source code of a page. It's exactly what your browser does when you open a website. To test it, open your browser, right-click anywhere on the page and select `view page source`. This is what Python will automate !\n",
    "\n",
    "**Extraction**\n",
    "\n",
    "From the `html` page you can extract, transform and load information in Python exactly like we did before with `xml`.\n",
    "\n",
    "Let's implement those two steps in few lines of code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching: Scraping via HTTP requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTTP (HyperText Transfer Protocol) is a protocol that will allow a **client** (you, through your browser or your code for example) to communicate with a **server** connected to the network (hosting a website or an internet document)\n",
    "\n",
    "Requests always go in pairs: the request (from the client) and the response (from the server).\n",
    "If this is not the case, it is because a problem has occurred at a point in the network.\n",
    "\n",
    "In Python we can use the library `requests` with the method `get`. It will emulate what you are doing manually in your browser.\n",
    "\n",
    "Start by installing the library using `pip install requests` or the `conda` package manager.\n",
    "\n",
    "Then let's take an URL, download the content and display it to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Url of website\n",
    "url = \"https://www.becode.org/about/\"\n",
    "# I send my HTTP request with a \"GET\" to the site server to identify in the url\n",
    "r = requests.get(url)\n",
    "# I display the requested url and the return of the server\n",
    "print(url, r.status_code)\n",
    "# I will store the content of the website in a variable and print it\n",
    "content = r.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the content is not easily readable. But like `xml`, `html` is a structured representation of the content. We then can use Python for reading it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction using Beautifulsoup\n",
    "\n",
    "For parsing `xml`we have used `xpath`. This is also possible for `html` ([see here](https://python-docs.readthedocs.io/en/latest/scenarios/scrape.html)).\n",
    "\n",
    "However there is a more _user-friendly_ library in Python that does the job quite well: `beautifulsoup`. We will use it in this notebook.\n",
    "\n",
    "First install it using `pip install beautifulsoup4` or the `conda` package manager."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the website content we just scraped in a Beautifulsoup object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(content, \"html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `html` tree of the page is now loaded and we can access its information. For example we can get the title of the page by recovering the content of the tag `h1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tag in soup.find_all(\"h1\"):\n",
    "    # We only retrieve the content ==> .text\n",
    "    print(tag.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same with `h2` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, do the same with the `p` tags (that stand for _paragraphs_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Choose an URL, get its content by using `requests`. Load it by using `beautifulsoup`. Then create a summary of the page by printing the content of `h1`, the first `h2` and the first paragraph (`p`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well done, you did it ! Let's go with more advanced operation using `beautifulsoup`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infosessionvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "661c13da0699b4d3adfbe1192573631e3fbd9fa55405ad8c238e615a4e7e8a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
