{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targeted information retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen how to parse a webpage, which retrieves the information without distinction.\n",
    "\n",
    "But, in general, the purpose of scraping is to automate the collection of targeted information on the web.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to scrape the description of the latest movies released in theaters.\n",
    "\n",
    "So we'll go to the Allociné website and try to find the tags that will give me links to the specific pages of these movies to get their summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery of the url of the pages of films newly released in the theaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://www.allocine.fr/\"\n",
    "r = requests.get(url)\n",
    "print(url, r.status_code)\n",
    "soup = BeautifulSoup(r.content, \"html\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your web browser (Chrome, Firefox,...), you can use the \"inspect\" function (right click -> inspect) and drag your mouse to the areas of the page that interest you. At the same time you will see the part of the HTML script highlighted that corresponds to the area that interests you.\n",
    "\n",
    "That's how you find the tags that you are interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that the relative link of the web page specific to the new movie on the poster is stored in these tags:\n",
    "\n",
    "```html\n",
    "<a class=\"meta-title meta-title-link\" href=\"/film/fichefilm_gen_cfilm=235582.html\" title=\"Le Grand Bain\">Le Grand Bain</a>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in soup.find_all(\"a\"):\n",
    "    print(p.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the site is more difficult to \"extract\". Let's use much more specific parameters to the search function `find_all`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In addition to the tag `a`, which is easily identifiable, we notice some additional\n",
    "# information such as the value of the class variable of these identical tags.\n",
    "for elem in soup.find_all(\"a\", attrs={\"class\": \"meta-title meta-title-link\"}):\n",
    "    print(elem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recovery of `href`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have noticed the presence of `href` links to the pages that interest us. Let's go get them back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in soup.find_all(\"a\", attrs={\"class\": \"meta-title meta-title-link\"}):\n",
    "    print(elem.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you retrieve the titles for me by searching for `\"title\"` in the items of the previous list?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by building the URL that we will use to retrieve the summaries.\n",
    "\n",
    "Start by putting the `href` values in a list of links.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for elem in soup.find_all(\"a\", attrs={\"class\": \"meta-title meta-title-link\"}):\n",
    "    links.append(elem.get(\"href\"))\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The absolute URL of the movie pages we are looking for are built in this form: http://www.allocine.fr/film/fichefilm_gen_cfilm=243835.html\n",
    "\n",
    "It is therefore necessary to take the previous list and build the absolute URLs for our search\n",
    "It's up to you to play.\n",
    "\n",
    "NB: Do not take the links for the shows(series). We only want movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links = [\"http://www.allocine.fr\" + elem for elem in links if \"film\" in elem]\n",
    "movie_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, on each page, the title and synopsis must be retrieved. Let's try for a movie, the first of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = movie_links[0]\n",
    "r = requests.get(url)\n",
    "print(url, r.status_code)\n",
    "soup = BeautifulSoup(r.content, \"html\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For title \n",
    "```html\n",
    "<div class=\"titlebar-title titlebar-title-lg\">Le Grand Bain</div>\n",
    "```\n",
    "For the synopsis\n",
    "\n",
    "```html\n",
    "<section class=\"section ovw ovw-synopsis\" id=\"synopsis-details\">\n",
    "\n",
    "      <div class=\"content-txt\">\n",
    " \n",
    "              C’est dans les couloirs de leur piscine municipale que Bertrand, Marcus, Simon, Laurent, Thierry et les autres s’entraînent sous l’autorité toute relative de Delphine, ancienne gloire des bassins. Ensemble, ils se sentent libres et utiles. Ils vont mettre toute leur énergie dans une discipline jusque-là propriété de la gent féminine : la natation synchronisée. Alors, oui c’est une idée plutôt bizarre, mais ce défi leur permettra de trouver un sens à leur vie...\n",
    "      </div>\n",
    "    \n",
    "      </section>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in soup.find_all(\"div\", attrs={\"class\": \"titlebar-title titlebar-title-lg\"}):\n",
    "    # Just like that\n",
    "    print(elem.text)\n",
    "\n",
    "# Get the synopsis section\n",
    "for elem in soup.find_all(\"section\", attrs={\"id\": \"synopsis-details\"}):\n",
    "    # Get the text of the synopsis\n",
    "    for elem2 in elem.find_all(\"div\", attrs={\"class\":\"content-txt\"}):\n",
    "        # Just like that\n",
    "        print(elem2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Automate this script for the entire list of movies.\n",
    "\n",
    "2) Put the information in three lists (`film_links`, `title`, `synopsis`).\n",
    "\n",
    "3) Create a `DataFrame` object from the `pandas` library that you will have to install. This dataframe will have to include these three informations in three columns.\n",
    "\n",
    "4) Save this dataframe in a CSV file.\n",
    "\n",
    "And here's your first real scrap, you're real hackers now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from random import randint\n",
    "\n",
    "title = []\n",
    "synopsis = []\n",
    "\n",
    "# Here are the things you will have to do for all links:\n",
    "# - Slow down the frequency of requests to avoid being identified and therefore banned from the website.\n",
    "# Use `time.sleep(random.uniform(1.0, 2.0))`\n",
    "# - Get request object from URL\n",
    "# - Extract the content into a variable using BeautifulSoup\n",
    "# - Get title\n",
    "# - Get synopsis\n",
    "\n",
    "# Check the length of the lists before creating the dataframe\n",
    "len(title), len(synopsis), len(movie_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\"Title\": title})\n",
    "df[\"Synopsis\"] = synopsis\n",
    "df[\"Links\"] = movie_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"./assets/allo_cine.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('infosessionvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "661c13da0699b4d3adfbe1192573631e3fbd9fa55405ad8c238e615a4e7e8a33"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
