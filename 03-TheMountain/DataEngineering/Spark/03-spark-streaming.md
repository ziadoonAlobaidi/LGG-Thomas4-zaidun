# Data Streaming with PySpark

![Data streaming pipeline](https://learn.microsoft.com/fr-fr/azure/hdinsight/spark/media/apache-spark-streaming-overview/hdinsight-spark-streaming.png)

Data streaming refers to the continuous flow of data that is generated by various sources, such as sensors, social media feeds, and financial transactions. 
In PySpark, you can use the streaming module to process data streams in real-time.

## 1. Connection to the data stream
To get started with data streaming in PySpark, you will need to create a `SparkSession` and a `StreamingContext`. 

The `StreamingContext` represents the connection to the data stream and allows you to create `DStream` (discretized stream) objects, which are small batches of data that are processed in parallel. 

You can create a `StreamingContext` by using the following code:

```python
from pyspark.sql import SparkSession
from pyspark.streaming import StreamingContext

# Create a SparkSession
spark = SparkSession.builder \
    .appName("My App") \
    .getOrCreate()

# Create a StreamingContext with a batch interval of 5 seconds
ssc = StreamingContext(spark.sparkContext, 5)
```

Next, you will need to specify the source of the data stream. 
There are many different sources you can use, such as [Kafka](https://kafka.apache.org/), [Flume](https://flume.apache.org/), and [Kinesis](https://aws.amazon.com/fr/kinesis/data-streams/). 

In this example, we will use a free data streaming API as the source. 
To do this, you can use the `socketTextStream` method, which creates a `DStream` that receives data over a network connection:

**THIS WILL NOT WORK YET*** We are gonna need to start the streaming, but we will do that later.

```python
# Create a DStream that receives data over a network connection
lines = ssc.socketTextStream("hostname", 9999)
```

## 2. data processing
Once you have a `DStream`, you can apply transformations to the data just like you would with a regular DataFrame. 

For example, you can split the lines of text into individual words and count the frequency of each word:

```python
# Split the lines into words and count the frequency of each word
word_counts = lines.flatMap(lambda line: line.split(" ")) \
                   .map(lambda word: (word, 1)) \
                   .reduceByKey(lambda a, b: a + b)

# Print the first 10 word counts
word_counts.pprint(10)
```

## 3. Start reading
Finally, you will need to start the `StreamingContext` to begin processing the data stream. You can do this by calling the `start()` method.
To stop the `StreamingContext`, you can call the `stop()` method:


```python
# Start the StreamingContext
ssc.start()
# Stop the StreamingContext
ssc.stop()
```